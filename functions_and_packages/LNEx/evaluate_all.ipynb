{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Hussein S. Al-Olimat, hussein@knoesis.org\n",
    "\n",
    "This software is released under the GNU Affero General Public License (AGPL) v3.0 License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate LNEx performance using the locations gold standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyHamcrest\n",
    "#!pip install wordsegment\n",
    "#!pip install shapely\n",
    "#!pip install nltk\n",
    "#!pip install elasticsearch\n",
    "#!pip install elasticsearch_dsl\n",
    "#!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import json, re, os\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import sys \n",
    "sys.path.append(\"LNEx\")\n",
    "import LNEx as lnex\n",
    "\n",
    "from shapely.geometry import MultiPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_they_overlap(tub1, tub2):\n",
    "    '''Checks whether two substrings of the tweet overlaps based on their start\n",
    "    and end offsets.'''\n",
    "\n",
    "    if tub2[1] >= tub1[0] and tub1[1] >= tub2[0]:\n",
    "        return True\n",
    "\n",
    "def read_annotations(filename):\n",
    "\n",
    "    filename = os.path.join(\"_Data/Brat_Annotations\", filename)\n",
    "\n",
    "    # read tweets from file to list\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return data\n",
    "\n",
    "def init_using_elasticindex(bb, cache, augmentType, dataset, capital_word_shape):\n",
    "    lnex.elasticindex(conn_string='localhost:9200', index_name=\"photon\")\n",
    "\n",
    "    geo_info = lnex.initialize( bb, augmentType=augmentType,\n",
    "                                    cache=cache,\n",
    "                                    dataset_name=dataset,\n",
    "                                    capital_word_shape=capital_word_shape)\n",
    "    return geo_info\n",
    "\n",
    "def evaluate(anns):\n",
    "    TPs_count = 0\n",
    "    FPs_count = 0\n",
    "    FNs_count = 0\n",
    "    overlaps_count = 0\n",
    "\n",
    "    fns = defaultdict(int)\n",
    "\n",
    "    count = 0\n",
    "    one_geolocation = 0\n",
    "    all_geolocation = 0\n",
    "    geo_codes_length_dist = defaultdict(int)\n",
    "\n",
    "    FPs_set = defaultdict(set)\n",
    "    FNs_set = defaultdict(set)\n",
    "\n",
    "    for key in list(anns.keys()):\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        # skip the development set\n",
    "        if dataset != \"houston\" and count < 500:\n",
    "            continue\n",
    "\n",
    "        tweet_lns = set()\n",
    "        lnex_lns = set()\n",
    "        tweet_text = \"\"\n",
    "\n",
    "        for ann in anns[key]:\n",
    "            if ann != \"text\":\n",
    "                ln = anns[key][ann]\n",
    "\n",
    "                tweet_lns.add(((int(ln['start_idx']), int(ln['end_idx'])),\n",
    "                                    ln['type']))\n",
    "            else:\n",
    "                tweet_text = anns[key][ann]\n",
    "\n",
    "                r = lnex.extract(tweet_text)\n",
    "\n",
    "                # how many are already disambiguated +++++++++++++++++++++++\n",
    "                for res in r:\n",
    "                    if len(res[3]) < 2:\n",
    "                        one_geolocation += 1\n",
    "\n",
    "                        #if len(res[3]) == 0:\n",
    "                            #print res[2]\n",
    "                    else:\n",
    "                        geo_codes_length_dist[len(res[3])] += 1\n",
    "\n",
    "                    all_geolocation += 1\n",
    "                # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "                lnex_lns = set([x[1] for x in r])\n",
    "\n",
    "        tweet_lns = set([x[0] for x in tweet_lns if x[1] == \"inLoc\"])\n",
    "\n",
    "        # True Positives +++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        TPs = tweet_lns.intersection(lnex_lns)\n",
    "\n",
    "        TPs_count += len(TPs)\n",
    "\n",
    "        # Left in both sets ++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        tweet_lns -= TPs\n",
    "        lnex_lns -= TPs\n",
    "\n",
    "        # Find Overlapping LNs to be counted as 1/2 FPs and 1/2 FNs++\n",
    "        overlaps = set()\n",
    "        for x in tweet_lns:\n",
    "            for y in lnex_lns:\n",
    "                if do_they_overlap(x, y):\n",
    "                    overlaps.add(x)\n",
    "                    overlaps.add(y)\n",
    "\n",
    "        overlaps_count += len(overlaps)\n",
    "\n",
    "        # remove the overlapping lns from lnex_lns and tweet_lns\n",
    "        lnex_lns -= overlaps\n",
    "        tweet_lns -= overlaps\n",
    "\n",
    "        # False Positives ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        # lnex_lns = all - (TPs and overlaps and !inLoc)\n",
    "        FPs = lnex_lns - tweet_lns\n",
    "        FPs_count += len(FPs)\n",
    "\n",
    "        if len(FPs) > 0:\n",
    "            for x in FPs:\n",
    "                FPs_set[tweet_text[x[0]:x[1]]].add((key,tweet_text[x[0]-2:x[1]+2],x))\n",
    "\n",
    "        # False Negatives ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        FNs = tweet_lns - lnex_lns\n",
    "        FNs_count += len(FNs)\n",
    "\n",
    "        if len(FNs) > 0:\n",
    "            for x in FNs:\n",
    "                FNs_set[tweet_text[x[0]:x[1]]].add((key,tweet_text[x[0]-2:x[1]+2],x))\n",
    "\n",
    "    '''\n",
    "    since we add 2 lns one from lnex_lns and one from tweet_lns if they\n",
    "    overlap the equation of counting those as 1/2 FPs and 1/2 FNs is going\n",
    "    to be:\n",
    "        overlaps_count x\n",
    "            1/2 (since we count twice) x\n",
    "                1/2 (since we want 1/2 of all the errors made)\n",
    "    '''\n",
    "\n",
    "    Precision = TPs_count/(TPs_count + FPs_count + 0.5 * .5 * overlaps_count)\n",
    "    Recall = TPs_count/(TPs_count + FNs_count + 0.5 * .5 * overlaps_count)\n",
    "    F_Score = (2 * Precision * Recall)/(Precision + Recall)\n",
    "\n",
    "    percentage_disambiguated = one_geolocation/all_geolocation\n",
    "\n",
    "    return {\"precision\": Precision, \n",
    "            \"recall\": Recall, \n",
    "            \"f-score\": F_Score}\n",
    "\n",
    "    #for x in FPs_set:\n",
    "    #    print (x, len(FPs_set[x]))\n",
    "\n",
    "    #for x in FNs_set:\n",
    "    #    print (x, len(FNs_set[x]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bbs = { \"chennai\": [12.74, 80.066986084, 13.2823848224, 80.3464508057],\n",
    "        \"louisiana\": [29.4563, -93.3453, 31.4521, -89.5276],\n",
    "        \"houston\": [29.4778611958, -95.975189209, 30.1463147381, -94.8889160156]}\n",
    "\n",
    "augmentTypes = [\"FILTER\", \"HP\", \"NA\"]# \"FULL\", \n",
    "\n",
    "for augmentType in augmentTypes:\n",
    "    print(augmentType)\n",
    "    \n",
    "    results = dict()\n",
    "    for dataset in bbs:\n",
    "\n",
    "        print(dataset)\n",
    "        geo_info = init_using_elasticindex(bbs[dataset], cache=False, augmentType=augmentType, \n",
    "                                           dataset=dataset, capital_word_shape=False)\n",
    "\n",
    "        filename = dataset+\"_annotations.json\"\n",
    "        anns = read_annotations(filename)\n",
    "\n",
    "        results[dataset] = evaluate(anns)\n",
    "\n",
    "    results[\"average-f\"] = sum([results[ds][\"f-score\"] for ds in results])/len(results)\n",
    "    pp.pprint(results)\n",
    "    print(\"#\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FULL\n",
    "\n",
    "{   'average-f': 0.8282360007690093,\n",
    "    'chennai': {   'f-score': 0.8900994789199431,\n",
    "                   'precision': 0.9470766129032258,\n",
    "                   'recall': 0.8395889186773905},\n",
    "    'houston': {   'f-score': 0.7369206180074587,\n",
    "                   'precision': 0.848068669527897,\n",
    "                   'recall': 0.651530852567122},\n",
    "    'louisiana': {   'f-score': 0.857687905379626,\n",
    "                     'precision': 0.895617529880478,\n",
    "                     'recall': 0.8228404099560761}}\n",
    "\n",
    "## FILTER\n",
    "\n",
    "{   'average-f': 0.8359112258688132,\n",
    "    'chennai': {   'f-score': 0.8766069926709119,\n",
    "                   'precision': 0.9362248171435904,\n",
    "                   'recall': 0.8241274144357845},\n",
    "    'houston': {   'f-score': 0.765867567852994,\n",
    "                   'precision': 0.8945355865301591,\n",
    "                   'recall': 0.6695595974517589},\n",
    "    'louisiana': {   'f-score': 0.8652591170825337,\n",
    "                     'precision': 0.9147727272727273,\n",
    "                     'recall': 0.820830298616169}}\n",
    "\n",
    "## HP\n",
    "\n",
    "{   'average-f': 0.836498466629482,\n",
    "    'chennai': {   'f-score': 0.8732905724313204,\n",
    "                   'precision': 0.942159550855203,\n",
    "                   'recall': 0.8138039923311153},\n",
    "    'houston': {   'f-score': 0.768642447418738,\n",
    "                   'precision': 0.904952476238119,\n",
    "                   'recall': 0.6680206794682423},\n",
    "    'louisiana': {   'f-score': 0.8675623800383877,\n",
    "                     'precision': 0.9186991869918699,\n",
    "                     'recall': 0.8218181818181818}}\n",
    "\n",
    "## NA\n",
    "\n",
    "{   'average-f': 0.7123791000175698,\n",
    "    'chennai': {   'f-score': 0.8141632837167342,\n",
    "                   'precision': 0.811965811965812,\n",
    "                   'recall': 0.8163726820443238},\n",
    "    'houston': {   'f-score': 0.6648441771459813,\n",
    "                   'precision': 0.6485333333333333,\n",
    "                   'recall': 0.6819966348850253},\n",
    "    'louisiana': {   'f-score': 0.658129839189994,\n",
    "                     'precision': 0.554718875502008,\n",
    "                     'recall': 0.8089311859443631}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
